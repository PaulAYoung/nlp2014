{
 "metadata": {
  "name": "",
  "signature": "sha256:f60976060f4631710e857e315a1296ba45e92a34344ec9ec6725519a3b4c1a4b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1. Common Objects of Verbs ##\n",
      "The Church and Hanks reading shows how interesting semantics can be found by looking at very simple patterns. For instance, if we look at what gets drunk (the object of the verb drink) we can automatically acquire a list of beverages. Similarly, if we find an informative verb in a text about mythology, and look at the subjects of certain verbs, we might be able to group all the gods' names together by seeing who does the blessing and smoting.\n",
      "More generally, looking at common objects of verbs, or in some cases, subjects of verbs, we have another piece of evidence for grouping similar words together.\n",
      "\n",
      "**Find frequent verbs:** Using your tagged collection from the previous assignment, first pull out verbs and then rank by frequency (if you like, you might use WordNet's morphy() to normalize them into their lemma form, but this is not required). Print out the top 40 most frequent verbs and take a look at them:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Pick 2 out interesting verbs:** Next manually pick out two verbs to look at in detail that look interesting to you. Try to pick some for which the objects will be interesting and will form a pattern of some kind.  Find all the sentences in your corpus that contain these verbs.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Find common objects:** Now write a chunker to find the simple noun phrase objects of these four verbs and see if they tell you anything interesting about your collection.  Don't worry about making the noun phrases perfect; you can use the chunker from the first part of this homework if you like.  Print out the common noun phrases and take a look.  Write the code below, show some of the output, and then reflect on that output in a few sentences.  \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2. Identify Main Topics from WordNet Hypernms ##\n",
      "First read about the code supplied below; at the end you'll be asked to do an exercise."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import wordnet as wn\n",
      "from nltk.corpus import brown\n",
      "from nltk.corpus import stopwords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This code first pulls out the most frequent words from a section of the brown corpus after removing stop words.  It lowercases everything, but should really be doing much smarter things with tokenization and phrases and so on. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def preprocess_terms():\n",
      "    # select a subcorpus of brown to experiment with\n",
      "    words = [word.lower() for word in brown.words(categories=\"science_fiction\") if word.lower() not in stopwords.words('english')]\n",
      "    # count up the words\n",
      "    fd = nltk.FreqDist(words)\n",
      "    # show some sample words\n",
      "    print ' '.join(fd.keys()[100:150])\n",
      "    return fd\n",
      "fd = preprocess_terms()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then makes a *very naive* guess at which are the most important words.  This is where some term weighting should take place."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_important_terms(fd):\n",
      "    important_words = fd.keys()[100:500]\n",
      "    return important_words\n",
      "\n",
      "important_terms = find_important_terms(fd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code below is a very crude way to see what the most common \"topics\" are among the \"important\" words, according to WordNet.  It does this by looking at the immediate hypernym of every sense of a wordform for those wordforms that are found to be nouns in WordNet.  This is problematic because many of these senses will be incorrect and also often the hypernym elides the specific meaning of the word, but if you compare, say *romance* to *science fiction* in brown, you do see differences in the results. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Count the direct hypernyms for every sense of each wordform.\n",
      "# This is very crude.  It should convert the wordform to a lemma, and should\n",
      "# be smarter about selecting important words and finding two-word phrases, etc.\n",
      "\n",
      "# Nonetheless, you get intersting differences between, say, scifi and romance.\n",
      "def categories_from_hypernyms(termlist):\n",
      "    hypterms = []                        \n",
      "    for term in termlist:                  # for each term\n",
      "        s = wn.synsets(term.lower(), 'n')  # get its nominal synsets\n",
      "        for syn in s:                      # for each synset\n",
      "            for hyp in syn.hypernyms():    # It has a list of hypernyms\n",
      "                hypterms = hypterms + [hyp.name]  # Extract the hypernym name and add to list\n",
      "\n",
      "    hypfd = nltk.FreqDist(hypterms)\n",
      "    print \"Show most frequent hypernym results\"\n",
      "    return [(count, name, wn.synset(name).definition) for (name, count) in hypfd.items()[:25]] \n",
      "    \n",
      "categories_from_hypernyms(important_terms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Here is the question** Modify this code in some way to do a better job of using WordNet to summarize terms.  You can trim senses in a better way, or traverse hypernyms differently.  You don't have to use hypernyms; you can use any WordNet relations you like, or chose your terms in another way.  You can also use other parts of speech if you like.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}