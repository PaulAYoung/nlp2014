{
 "metadata": {
  "gist_id": "18c23282dd08e6579dc5",
  "name": "",
  "signature": "sha256:ba552b36c2c5a8df5f17ce816cc90737e485b2ba831d60b4849cf80fe83367ee"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Keyphrase Identification\n",
      "\n",
      "This notebook will try several methods for identifying keyphrases in text. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Setup\n",
      "\n",
      "First, let's import modules, load text, and do the required tokenizing and tagging. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import brown\n",
      "from nltk.corpus import wordnet as wn\n",
      "\n",
      "import corpii"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from urllib import urlopen"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can uncomment some lines below to use a different text source. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = nltk.clean_html(corpii.load_pres_debates().raw())\n",
      "\n",
      "# code to get text from alternate source\n",
      "#text = urlopen(\"www.url.com\").read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's tokenize, split into sentences, and tag the text. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tokenize\n",
      "token_regex= \"\"\"(?x)\n",
      "    # taken from ntlk book example\n",
      "    ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
      "    | \\w+(-\\w+)*        # words with optional internal hyphens\n",
      "    | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
      "    | \\.\\.\\.            # ellipsis\n",
      "    | [][.,;\"'?():-_`]  # these are separate tokens\n",
      "\"\"\"\n",
      "\n",
      "tokens = nltk.regexp_tokenize(text, token_regex)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get sentences\n",
      "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "sents = list(sent_tokenizer.sentences_from_tokens(tokens))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Create tagger\n",
      "\n",
      "def build_backoff_tagger (train_sents):\n",
      "    t0 = nltk.DefaultTagger('NN')\n",
      "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
      "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
      "    return t2\n",
      "\n",
      "tagger = build_backoff_tagger(brown.tagged_sents())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent_tags = [tagger.tag(s) for s in sents]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tags = [t for s in sent_tags for t in s]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# we're going to use frequency distributions a lot, so let's create a nice way of looking at those\n",
      "\n",
      "def fd_view(fd, n):\n",
      "    \"\"\"Prints a nice format of items in FreqDist fd[0:n]\"\"\"\n",
      "    print \"{:<16}|{:<16}|{:<16}\".format(\"Word\", \"Count\", \"Frequency\")\n",
      "    print \"=========================================================\"\n",
      "    for i in fd.items()[0:n]:\n",
      "        print \"{:<16}|{:<16,d}|{:<16.3%}\".format(i[0], i[1], fd.freq(i[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Simple unigram approach\n",
      "\n",
      "To start let's try just finding the most common nouns in the corpus and see how that does. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def common_nouns(tags, min_length=4):\n",
      "    \"\"\"Takes a tagset and returns a frequency distribution of the nouns that are at least min_length\"\"\"\n",
      "    fd_nouns = nltk.FreqDist([ t[0].lower() for t in tags if len(t[0]) >= min_length and re.match(\"NN.*\", t[1])])\n",
      "    return fd_nouns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's look for common nouns\n",
      "# I was getting some noise from very short tokens, so I'm excluding them.\n",
      "fd_tokens = common_nouns(tags, 4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#First let's see what some of these tokens are\n",
      "fd_view(fd_tokens, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "president       |3,052           |2.673%          \n",
        "people          |2,601           |2.278%          \n",
        "senator         |1,265           |1.108%          \n",
        "years           |1,202           |1.053%          \n",
        "country         |1,173           |1.027%          \n",
        "question        |1,153           |1.010%          \n",
        "time            |1,046           |0.916%          \n",
        "governor        |1,028           |0.900%          \n",
        "states          |886             |0.776%          \n",
        "government      |856             |0.750%          \n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's actually not too bad for a corpus of presidential debates. \n",
      "\n",
      "Let's try the same thing on the brown news corpus. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd_brown = common_nouns(brown.tagged_words(categories='news'), 4)\n",
      "fd_view(fd_brown, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "state           |151             |0.740%          \n",
        "president       |142             |0.696%          \n",
        "year            |142             |0.696%          \n",
        "time            |103             |0.505%          \n",
        "years           |102             |0.500%          \n",
        "house           |94              |0.461%          \n",
        "week            |94              |0.461%          \n",
        "city            |93              |0.456%          \n",
        "school          |87              |0.426%          \n",
        "home            |76              |0.372%          \n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "hmmmmm... a lot of things that sound newsy in there. A little scattered, but the news corpus probably covers a lot of ground. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##A more complex approach with collocations\n",
      "\n",
      "Let's see if doing something more complex can beat the simple approach. One weakness with unigrams is that it only catches topics that are one word long. Let's try using collocations and PMI to see if we can find some interesting word combinations. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_measures = nltk.collocations.BigramAssocMeasures()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bigram_collocs(tokens, mfreq=3, measure=bigram_measures.pmi, n=10):\n",
      "    \"\"\"\n",
      "    Look for collocations in token list:tokens\n",
      "    \n",
      "    args:\n",
      "    mfreq - minimum frequency to be included\n",
      "    measure - The measure to be used to find collocations\n",
      "    n - The number of matches to show\n",
      "    \"\"\"\n",
      "    finder = nltk.BigramCollocationFinder.from_words(tokens)\n",
      "    finder.apply_freq_filter(mfreq)\n",
      "    return finder.nbest(measure, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_collocs(tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "[('A.Q.', 'Khan'),\n",
        " ('Achilles', 'heel'),\n",
        " ('Beta', 'Kappa'),\n",
        " ('Costa', 'Rica'),\n",
        " ('Dana', 'Crist'),\n",
        " ('EXECUTIVE', 'EDITOR'),\n",
        " ('Helping', 'Hand'),\n",
        " ('NEW', 'YORK'),\n",
        " ('Occupational', 'Safety'),\n",
        " ('Phi', 'Beta')]"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_collocs(brown.words(categories=['news']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "[('Sterling', 'Township'),\n",
        " ('Duncan', 'Phyfe'),\n",
        " ('Milwaukee', 'Braves'),\n",
        " ('magnetic', 'tape'),\n",
        " ('Dolce', 'Vita'),\n",
        " ('Notre', 'Dame'),\n",
        " ('Scottish', 'Rite'),\n",
        " ('Thrift', 'Shop'),\n",
        " ('Adlai', 'Stevenson'),\n",
        " ('Lady', 'Jacqueline')]"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That isn't too helpful, maybe the chi measure?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_collocs(tokens, measure=bigram_measures.chi_sq)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "[('Left', 'Behind'),\n",
        " ('Los', 'Angeles'),\n",
        " ('Prime', 'Minister'),\n",
        " ('El', 'Salvador'),\n",
        " ('Chiang', 'Kai-shek'),\n",
        " ('Jong', 'Il'),\n",
        " ('Training', 'Partnership'),\n",
        " ('et', 'cetera'),\n",
        " ('PARTICIPATE', 'IN'),\n",
        " ('Planned', 'Parenthood')]"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_collocs(brown.words(categories=['news']), measure=bigram_measures.chi_sq)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "[('Viet', 'Nam'),\n",
        " ('Hong', 'Kong'),\n",
        " ('Dolce', 'Vita'),\n",
        " ('Notre', 'Dame'),\n",
        " ('Scottish', 'Rite'),\n",
        " ('Duncan', 'Phyfe'),\n",
        " ('Sterling', 'Township'),\n",
        " ('Los', 'Angeles'),\n",
        " ('per', 'cent'),\n",
        " ('Thrift', 'Shop')]"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I think one problem with applying the collocations metric to my corpus is that it is very large and covers several debates where different topics are important, so it doesn't create a very coherent result. I will try running it against individual debates to see if that's more interesting. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "debates = corpii.load_pres_debates()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for debate in debates.fileids():\n",
      "    d = nltk.clean_html(debates.raw(fileids=[debate]))\n",
      "    d_tokens = nltk.regexp_tokenize(debate, token_regex)\n",
      "    collocs = bigram_collocs(d_tokens, n=5, mfreq=2)\n",
      "    if collocs:\n",
      "        print collocs\n",
      "    else:\n",
      "        print \"No collocations found in {}\".format(debate)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No collocations found in First_half_of_Debate.txt\n",
        "No collocations found in October_11,_1984:_The_Bush-Ferraro_Vice_Presidential_Debate.txt\n",
        "No collocations found in October_11,_2000:_The_Second_Gore-Bush_Presidential_Debate.txt\n",
        "No collocations found in October_11,_2012:_The_Biden-Ryan_Vice_Presidential_Debate.txt\n",
        "No collocations found in October_13,_1960:_The_Third_Kennedy-Nixon_Presidential_Debate.txt\n",
        "No collocations found in October_13,_1988:_The_Second_Bush-Dukakis_Presidential_Debate.txt\n",
        "No collocations found in October_13,_1992:_The_Gore-Quayle-Stockdale_Vice_Presidential_Debate.txt\n",
        "No collocations found in October_13,_2004:_The_Third_Bush-Kerry_Presidential_Debate.txt\n",
        "No collocations found in October_15,_2008:_The_Third_McCain-Obama_Presidential_Debate.txt\n",
        "No collocations found in October_16,_1996:_The_Second_Clinton-Dole_Presidential_Debate.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No collocations found in October_16,_2012:_The_Second_Obama-Romney_Presidential_Debate.txt\n",
        "No collocations found in October_17,_2000:_The_Third_Gore-Bush_Presidential_Debate.txt\n",
        "No collocations found in October_19,_1992:_The_Third_Clinton-Bush-Perot_Presidential_Debate.txt\n",
        "No collocations found in October_2,_2008:_The_Biden-Palin_Vice_Presidential_Debate.txt\n",
        "No collocations found in October_21,_1960:_The_Fourth_Kennedy-Nixon_Presidential_Debate.txt\n",
        "No collocations found in October_21,_1984:_The_Second_Reagan-Mondale_Presidential_Debate.txt\n",
        "No collocations found in October_22,_1976:_The_Third_Carter-Ford_Presidential_Debate.txt\n",
        "No collocations found in October_22,_2012:_The_Third_Obama-Romney_Presidential_Debate.txt\n",
        "No collocations found in October_28,_1980:_The_Carter-Reagan_Presidential_Debate.txt\n",
        "No collocations found in October_3,_2000:_The_First_Gore-Bush_Presidential_Debate.txt\n",
        "No collocations found in October_3,_2012:_The_First_Obama-Romney_Presidential_Debate.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No collocations found in October_5,_1988:_The_Bentsen-Quayle_Vice_Presidential_Debate.txt\n",
        "No collocations found in October_5,_2000:_The_Lieberman-Cheney_Vice_Presidential_Debate.txt\n",
        "No collocations found in October_5,_2004:_The_Cheney-Edwards_Vice_Presidential_Debate.txt\n",
        "No collocations found in October_6,_1976:_The_Second_Carter-Ford_Presidential_Debate.txt\n",
        "No collocations found in October_6,_1996:_The_First_Clinton-Dole_Presidential_Debate.txt\n",
        "No collocations found in October_7,_1960:_The_Second_Kennedy-Nixon_Presidential_Debate.txt\n",
        "No collocations found in October_7,_1984:_The_First_Reagan-Mondale_Presidential_Debate.txt\n",
        "No collocations found in October_7,_2008:_The_Second_McCain-Obama_Presidential_Debate.txt\n",
        "No collocations found in October_8,_2004:_The_Second_Bush-Kerry_Presidential_Debate.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No collocations found in October_9,_1996:_The_Gore-Kemp_Vice_Presidential_Debate.txt\n",
        "No collocations found in Second_half_of_Debate.txt\n",
        "No collocations found in September_21,_1980:_The_Anderson-Reagan_Presidential_Debate.txt\n",
        "No collocations found in September_23,_1976:_The_First_Carter-Ford_Presidential_Debate.txt\n",
        "No collocations found in September_25,_1988:_The_First_Bush-Dukakis_Presidential_Debate.txt\n",
        "No collocations found in September_26,_1960:_The_First_Kennedy-Nixon_Presidential_Debate.txt\n",
        "No collocations found in September_26,_2008:_The_First_McCain-Obama_Presidential_Debate.txt\n",
        "No collocations found in September_30,_2004:_The_First_Bush-Kerry_Presidential_Debate.txt\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Well that didn't work, even with a lowered minimum frequency. (There are results when mfreq is lowered to 1, but they're pretty useless)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Gleaning topics from unigrams\n",
      "\n",
      "Let's go back to the unigrams and see if we can use wordnet to abstract the most frequent nouns into concepts. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_hypernyms(synsets):\n",
      "    \"\"\"\n",
      "    Takes a list of synsets (as generated by wn.synsets) and returns a list of all hypernyms. \n",
      "    \"\"\"\n",
      "    hypernyms = set()\n",
      "    for synset in synsets:\n",
      "        for path in synset.hypernym_paths():\n",
      "            hypernyms.update([h for h in path if h != synset])\n",
      "    return hypernyms"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def common_hypernyms(wordforms, min_depth=3):\n",
      "    \"\"\"\n",
      "    Takes a list of wordforms and extracts all hypernyms associated with the wordforms. \n",
      "    Returns a frequency distribution of of the sysnsets extracted. \n",
      "    arguments:\n",
      "        wordforms - Wordforms to be processed\n",
      "        min_depth - A filter to only include synsets of a certain depth.\n",
      "                    Unintuitively, max_depth is used to calculate the depth of a synset. \n",
      "    \"\"\"\n",
      "    hypernyms = []\n",
      "    for l in wordforms:\n",
      "        hset = get_hypernyms(wn.synsets(l, pos=wn.NOUN))\n",
      "        hypernyms.extend(h for h in hset if h.max_depth()>=min_depth)\n",
      "    return nltk.FreqDist(hypernyms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fd_hypernyms(fd, depth=None, min_depth=3, pos=None):\n",
      "    \"\"\"\n",
      "    Takes a frequency distribution and analyzes the hypernyms of the wordforms contained therein. \n",
      "    Returns a weighted \n",
      "    fd - frequency distribution\n",
      "    depth - How far down fd to look\n",
      "    min_depth - A filter to only include synsets of a certain depth.\n",
      "                Unintuitively, max_depth is used to calculate the depth of a synset. \n",
      "    pos - part of speech to limit sysnsets to\n",
      "    \"\"\"\n",
      "    hypernyms = {}\n",
      "    for wf in fd.keys()[0:depth]:\n",
      "        freq = fd.freq(wf)\n",
      "        hset = get_hypernyms(wn.synsets(wf, pos=pos))\n",
      "        for h in hset:\n",
      "            if h.max_depth()>=min_depth:\n",
      "                if h in hypernyms:\n",
      "                    hypernyms[h] += freq\n",
      "                else:\n",
      "                    hypernyms[h] = freq\n",
      "    \n",
      "    hlist = hypernyms.items()\n",
      "    hlist.sort(key=lambda s: s[1], reverse=True)\n",
      "    return hlist\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "debate_concepts = fd_hypernyms(fd_tokens, pos=wn.NOUN, min_depth=7)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[s[0].definition for s in debate_concepts[0:10]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "['a person who rules or guides or inspires others',\n",
        " 'a person who communicates with others',\n",
        " 'someone who negotiates (confers with others in order to reach a settlement)',\n",
        " 'a person who represents others',\n",
        " 'the chief public representative of a country who may also be the head of government',\n",
        " 'a person who is in charge',\n",
        " 'someone who administers a business',\n",
        " 'the leader of a group meeting',\n",
        " 'a person responsible for the administration of a business',\n",
        " 'an administrative unit in government or business']"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def concept_printer(concepts, n=10):\n",
      "    \"Prints first n concepts in concept list generated by fd_hypernyms\"\n",
      "    print \"{:<20} | {:<10} | {}\".format(\"Concept\", \"Noun Freq\", \"Definition\")\n",
      "    print \"====================================================================\"\n",
      "    for s in debate_concepts[0:10]:\n",
      "        print \"{:<20} | {:<10.3%} |  {}\".format(s[0].lemma_names[0], s[1], s[0].definition)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "concept_printer(debate_concepts, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Concept              | Noun Freq  | Definition\n",
        "====================================================================\n",
        "leader               | 7.989%     |  a person who rules or guides or inspires others\n",
        "communicator         | 5.141%     |  a person who communicates with others\n",
        "negotiator           | 4.295%     |  someone who negotiates (confers with others in order to reach a settlement)\n",
        "representative       | 3.958%     |  a person who represents others\n",
        "head_of_state        | 3.909%     |  the chief public representative of a country who may also be the head of government\n",
        "head                 | 3.396%     |  a person who is in charge\n",
        "administrator        | 3.119%     |  someone who administers a business\n",
        "presiding_officer    | 3.073%     |  the leader of a group meeting\n",
        "executive            | 3.050%     |  a person responsible for the administration of a business\n",
        "division             | 2.969%     |  an administrative unit in government or business\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "syn = debate_concepts[2][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "syn.lemma_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 51,
       "text": [
        "['negotiator', 'negotiant', 'treater']"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brown_concepts = fd_hypernyms(fd_brown, pos=wn.NOUN, min_depth=7)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "concept_printer(brown_concepts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Concept              | Noun Freq  | Definition\n",
        "====================================================================\n",
        "leader               | 7.989%     |  a person who rules or guides or inspires others\n",
        "communicator         | 5.141%     |  a person who communicates with others\n",
        "negotiator           | 4.295%     |  someone who negotiates (confers with others in order to reach a settlement)\n",
        "representative       | 3.958%     |  a person who represents others\n",
        "head_of_state        | 3.909%     |  the chief public representative of a country who may also be the head of government\n",
        "head                 | 3.396%     |  a person who is in charge\n",
        "administrator        | 3.119%     |  someone who administers a business\n",
        "presiding_officer    | 3.073%     |  the leader of a group meeting\n",
        "executive            | 3.050%     |  a person responsible for the administration of a business\n",
        "division             | 2.969%     |  an administrative unit in government or business\n"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Chunking Noun Phrases\n",
      "\n",
      "I want to try and pull out noun phrases, as I think that is pretty important for my corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grammar = r\"\"\"\n",
      "    NPHRASE: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and nouns\n",
      "             {<NN>+}                # chunk sequences of nouns\n",
      "    NPROP: {<NP>+|<NP><IN.*|DT.*><NP>}\n",
      "\"\"\"\n",
      "\n",
      "noun_parser = nltk.RegexpParser(grammar)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "debate_nps = [noun_parser.parse(s) for s in sent_tags]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for t in debate_nps[0:20]:\n",
      "    print [s for s in t.subtrees() if s.node!=\"S\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Tree('NPROP', [('October', 'NP')]), Tree('NPHRASE', [('1992', 'NN')]), Tree('NPHRASE', [('Debate', 'NN')]), Tree('NPHRASE', [('Transcript', 'NN')]), Tree('NPROP', [('October', 'NP')]), Tree('NPHRASE', [('1992', 'NN')]), Tree('NPHRASE', [('Clinton-Bush-Perot', 'NN')]), Tree('NPHRASE', [('Debate', 'NN')]), Tree('NPHRASE', [('Debate', 'NN')]), Tree('NPHRASE', [('transcript', 'NN')]), Tree('NPHRASE', [('debate', 'NN')])]\n",
        "[Tree('NPROP', [('October', 'NP')]), Tree('NPHRASE', [('\"', 'NN')]), Tree('NPHRASE', [('town', 'NN')]), Tree('NPHRASE', [('hall', 'NN')]), Tree('NPHRASE', [('\"', 'NN')]), Tree('NPHRASE', [('format', 'NN')]), Tree('NPHRASE', [('debate', 'NN')]), Tree('NPHRASE', [('moderated', 'NN')]), Tree('NPHRASE', [('Carole', 'NN')]), Tree('NPROP', [('Simpson', 'NP')])]\n",
        "[Tree('NPHRASE', [('format', 'NN')])]\n",
        "[Tree('NPHRASE', [('length', 'NN')]), Tree('NPHRASE', [('transcript', 'NN')])]\n",
        "[Tree('NPHRASE', [('CAROLE', 'NN')]), Tree('NPHRASE', [('SIMPSON', 'NN')]), Tree('NPHRASE', [('Good', 'JJ'), ('evening', 'NN')]), Tree('NPHRASE', [('welcome', 'NN')]), Tree('NPHRASE', [('president', 'NN')]), Tree('NPHRASE', [('US', 'NN')])]\n",
        "[Tree('NPROP', [('Republican', 'NP')]), Tree('NPHRASE', [('nominee', 'NN')]), Tree('NPROP', [('George', 'NP'), ('Bush', 'NP')]), Tree('NPROP', [('Ross', 'NP')]), Tree('NPHRASE', [('Perot', 'NN')]), Tree('NPROP', [('Bill', 'NP'), ('Clinton', 'NP')]), Tree('NPHRASE', [('nominee', 'NN')])]\n",
        "[Tree('NPHRASE', [('My', 'PP$'), ('name', 'NN')]), Tree('NPHRASE', [('Carole', 'NN')]), Tree('NPROP', [('Simpson', 'NP')]), Tree('NPHRASE', [('moderator', 'NN')]), Tree('NPHRASE', [('s', 'NN')]), Tree('NPHRASE', [('90-minute', 'NN')]), Tree('NPHRASE', [('debate', 'NN')]), Tree('NPHRASE', [('campus', 'NN')]), Tree('NPROP', [('Richmond', 'NP')]), Tree('NPROP', [('Richmond', 'NP')]), Tree('NPROP', [('Virginia', 'NP')])]\n",
        "[Tree('NPHRASE', [('s', 'NN')]), Tree('NPHRASE', [('program', 'NN')]), Tree('NPHRASE', [('presidential', 'JJ'), ('debate', 'NN')]), Tree('NPHRASE', [('history', 'NN')])]\n",
        "[Tree('NPHRASE', [('re', 'NN')]), Tree('NPHRASE', [('history', 'NN')]), Tree('NPHRASE', [('s', 'NN')])]\n",
        "[Tree('NPHRASE', [('firm', 'NN')]), Tree('NPHRASE', [('audience', 'NN')]), Tree('NPHRASE', [('209', 'NN')]), Tree('NPHRASE', [('this', 'DT'), ('area', 'NN')])]\n",
        "[Tree('NPHRASE', [('topic', 'NN')])]\n",
        "[Tree('NPHRASE', [('My', 'PP$'), ('job', 'NN')]), Tree('NPHRASE', [('moderator', 'NN')]), Tree('NPHRASE', [('care', 'NN')]), Tree('NPHRASE', [('continuity', 'NN')]), Tree('NPHRASE', [('another', 'DT'), ('candidate', 'NN')])]\n",
        "[Tree('NPHRASE', [('format', 'NN')]), Tree('NPROP', [('Republican', 'NP')]), Tree('NPHRASE', [('subject', 'NN')]), Tree('NPHRASE', [('matter', 'NN')])]\n",
        "[]\n",
        "[]\n",
        "[Tree('NPHRASE', [('debate', 'NN')]), Tree('NPHRASE', [('opportunity', 'NN')]), Tree('NPHRASE', [('statement', 'NN')])]\n",
        "[Tree('NPROP', [('Bush', 'NP')]), Tree('NPHRASE', [('s', 'NN')])]\n",
        "[Tree('NPHRASE', [('PRESIDENT', 'NN')]), Tree('NPHRASE', [('GEORGE', 'NN')]), Tree('NPHRASE', [('BUSH', 'NN')]), Tree('NPHRASE', [('s', 'NN')])]\n",
        "[Tree('NPHRASE', [('SIMPSON', 'NN')]), Tree('NPHRASE', [('question', 'NN')])]\n",
        "[Tree('NPHRASE', [('AUDIENCE', 'NN')]), Tree('NPHRASE', [('QUESTION', 'NN')])]\n"
       ]
      }
     ],
     "prompt_number": 76
    }
   ],
   "metadata": {}
  }
 ]
}