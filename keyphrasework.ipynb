{
 "metadata": {
  "gist_id": "18c23282dd08e6579dc5",
  "name": "",
  "signature": "sha256:8f35a2640cb030373c39482cd9e0260684167331edbc9fac98ce85e57cf36869"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Keyphrase Identification\n",
      "\n",
      "This notebook will try several methods for identifying keyphrases in text. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Setup\n",
      "\n",
      "First, let's import modules, load text, and do the required tokenizing and tagging. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import brown\n",
      "from nltk.corpus import wordnet as wn\n",
      "\n",
      "import corpii"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from urllib import urlopen"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can uncomment some lines below to use a different text source. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = nltk.clean_html(corpii.load_pres_debates().raw())\n",
      "\n",
      "# code to get text from alternate source\n",
      "#text = urlopen(\"www.url.com\").read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's tokenize, split into sentences, and tag the text. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tokenize\n",
      "token_regex= \"\"\"(?x)\n",
      "    # taken from ntlk book example\n",
      "    ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
      "    | \\w+(-\\w+)*        # words with optional internal hyphens\n",
      "    | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
      "    | \\.\\.\\.            # ellipsis\n",
      "    | [][.,;\"'?():-_`]  # these are separate tokens\n",
      "\"\"\"\n",
      "\n",
      "tokens = nltk.regexp_tokenize(text, token_regex)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get sentences\n",
      "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "sents = list(sent_tokenizer.sentences_from_tokens(tokens))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Create tagger\n",
      "\n",
      "def build_backoff_tagger (train_sents):\n",
      "    t0 = nltk.DefaultTagger('NN')\n",
      "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
      "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
      "    return t2\n",
      "\n",
      "tagger = build_backoff_tagger(brown.tagged_sents())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent_tags = [tagger.tag(s) for s in sents]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tags = [t for s in sent_tags for t in s]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# we're going to use frequency distributions a lot, so let's create a nice way of looking at those\n",
      "\n",
      "def fd_view(fd, n=10):\n",
      "    \"\"\"Prints a nice format of items in FreqDist fd[0:n]\"\"\"\n",
      "    print \"{:<16}|{:<16}|{:<16}\".format(\"Word\", \"Count\", \"Frequency\")\n",
      "    print \"=========================================================\"\n",
      "    for i in fd.items()[0:n]:\n",
      "        print \"{:<16}|{:<16,d}|{:<16.3%}\".format(i[0], i[1], fd.freq(i[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Simple unigram approach\n",
      "\n",
      "To start let's try just finding the most common nouns in the corpus and see how that does. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def common_nouns(tags, min_length=4, pos=r\"N.*\"):\n",
      "    \"\"\"Takes a tagset and returns a frequency distribution of the words\n",
      "    that are at least min_length and whose tag matches pos\"\"\"\n",
      "    fd_nouns = nltk.FreqDist([ t[0].lower() for t in tags if len(t[0]) >= min_length and re.match(pos, t[1])])\n",
      "    return fd_nouns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's look for common nouns\n",
      "# I was getting some noise from very short tokens, so I'm excluding them.\n",
      "fd_tokens = common_nouns(tags, 4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#First let's see what some of these tokens are\n",
      "fd_view(fd_tokens, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "president       |3,052           |2.409%          \n",
        "people          |2,601           |2.053%          \n",
        "senator         |1,265           |0.998%          \n",
        "years           |1,202           |0.949%          \n",
        "country         |1,173           |0.926%          \n",
        "question        |1,153           |0.910%          \n",
        "time            |1,046           |0.826%          \n",
        "governor        |1,028           |0.811%          \n",
        "america         |1,025           |0.809%          \n",
        "bush            |937             |0.740%          \n"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's actually not too bad for a corpus of presidential debates. \n",
      "\n",
      "Let's try the same thing on the brown news corpus. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd_brown = common_nouns(brown.tagged_words(categories='news'), 4)\n",
      "fd_view(fd_brown, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "mrs.            |253             |0.899%          \n",
        "state           |151             |0.537%          \n",
        "president       |142             |0.505%          \n",
        "year            |142             |0.505%          \n",
        "home            |132             |0.469%          \n",
        "time            |103             |0.366%          \n",
        "years           |102             |0.362%          \n",
        "house           |96              |0.341%          \n",
        "week            |94              |0.334%          \n",
        "city            |93              |0.330%          \n"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "hmmmmm... a lot of things that sound newsy in there. A little scattered, but the news corpus probably covers a lot of ground. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##A more complex approach with collocations\n",
      "\n",
      "Let's see if doing something more complex can beat the simple approach. One weakness with unigrams is that it only catches topics that are one word long. Let's try using collocations and PMI to see if we can find some interesting word combinations. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_measures = nltk.collocations.BigramAssocMeasures()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bigram_collocs(tokens, mfreq=3, measure=bigram_measures.pmi, n=10):\n",
      "    \"\"\"\n",
      "    Look for collocations in token list:tokens\n",
      "    \n",
      "    args:\n",
      "    mfreq - minimum frequency to be included\n",
      "    measure - The measure to be used to find collocations\n",
      "    n - The number of matches to show\n",
      "    \"\"\"\n",
      "    finder = nltk.BigramCollocationFinder.from_words(tokens)\n",
      "    finder.apply_freq_filter(mfreq)\n",
      "    return finder.nbest(measure, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_collocs(tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "[('A.Q.', 'Khan'),\n",
        " ('Achilles', 'heel'),\n",
        " ('Beta', 'Kappa'),\n",
        " ('Costa', 'Rica'),\n",
        " ('Dana', 'Crist'),\n",
        " ('EXECUTIVE', 'EDITOR'),\n",
        " ('Helping', 'Hand'),\n",
        " ('NEW', 'YORK'),\n",
        " ('Occupational', 'Safety'),\n",
        " ('Phi', 'Beta')]"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_collocs(brown.words(categories=['news']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "[('Sterling', 'Township'),\n",
        " ('Duncan', 'Phyfe'),\n",
        " ('Milwaukee', 'Braves'),\n",
        " ('magnetic', 'tape'),\n",
        " ('Dolce', 'Vita'),\n",
        " ('Notre', 'Dame'),\n",
        " ('Scottish', 'Rite'),\n",
        " ('Thrift', 'Shop'),\n",
        " ('Adlai', 'Stevenson'),\n",
        " ('Lady', 'Jacqueline')]"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That isn't too helpful, maybe the chi measure?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_collocs(tokens, measure=bigram_measures.chi_sq)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "[('Left', 'Behind'),\n",
        " ('Los', 'Angeles'),\n",
        " ('Prime', 'Minister'),\n",
        " ('El', 'Salvador'),\n",
        " ('Chiang', 'Kai-shek'),\n",
        " ('Jong', 'Il'),\n",
        " ('Training', 'Partnership'),\n",
        " ('et', 'cetera'),\n",
        " ('PARTICIPATE', 'IN'),\n",
        " ('Planned', 'Parenthood')]"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_collocs(brown.words(categories=['news']), measure=bigram_measures.chi_sq)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "[('Viet', 'Nam'),\n",
        " ('Hong', 'Kong'),\n",
        " ('Dolce', 'Vita'),\n",
        " ('Notre', 'Dame'),\n",
        " ('Scottish', 'Rite'),\n",
        " ('Duncan', 'Phyfe'),\n",
        " ('Sterling', 'Township'),\n",
        " ('Los', 'Angeles'),\n",
        " ('per', 'cent'),\n",
        " ('Thrift', 'Shop')]"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I think one problem with applying the collocations metric to my corpus is that it is very large and covers several debates where different topics are important, so it doesn't create a very coherent result. I will try running it against individual debates to see if that's more interesting. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "debates = corpii.load_pres_debates()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for debate in debates.fileids():\n",
      "    d = nltk.clean_html(debates.raw(fileids=[debate]))\n",
      "    d_tokens = nltk.regexp_tokenize(debate, token_regex)\n",
      "    collocs = bigram_collocs(d_tokens, n=5, mfreq=2)\n",
      "    if collocs:\n",
      "        print collocs\n",
      "    else:\n",
      "        print \"No collocations found in {}\".format(debate)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No collocations found in First_half_of_Debate.txt\n",
        "No collocations found in October_11,_1984:_The_Bush-Ferraro_Vice_Presidential_Debate.txt\n",
        "No collocations found in October_11,_2000:_The_Second_Gore-Bush_Presidential_Debate.txt\n",
        "No collocations found in October_11,_2012:_The_Biden-Ryan_Vice_Presidential_Debate.txt\n",
        "No collocations found in October_13,_1960:_The_Third_Kennedy-Nixon_Presidential_Debate.txt\n",
        "No collocations found in October_13,_1988:_The_Second_Bush-Dukakis_Presidential_Debate.txt\n",
        "No collocations found in October_13,_1992:_The_Gore-Quayle-Stockdale_Vice_Presidential_Debate.txt\n",
        "No collocations found in October_13,_2004:_The_Third_Bush-Kerry_Presidential_Debate.txt\n",
        "No collocations found in October_15,_2008:_The_Third_McCain-Obama_Presidential_Debate.txt\n",
        "No collocations found in October_16,_1996:_The_Second_Clinton-Dole_Presidential_Debate.txt\n",
        "No collocations found in October_16,_2012:_The_Second_Obama-Romney_Presidential_Debate.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No collocations found in October_17,_2000:_The_Third_Gore-Bush_Presidential_Debate.txt\n",
        "No collocations found in October_19,_1992:_The_Third_Clinton-Bush-Perot_Presidential_Debate.txt\n",
        "No collocations found in October_2,_2008:_The_Biden-Palin_Vice_Presidential_Debate.txt\n",
        "No collocations found in October_21,_1960:_The_Fourth_Kennedy-Nixon_Presidential_Debate.txt\n",
        "No collocations found in October_21,_1984:_The_Second_Reagan-Mondale_Presidential_Debate.txt\n",
        "No collocations found in October_22,_1976:_The_Third_Carter-Ford_Presidential_Debate.txt\n",
        "No collocations found in October_22,_2012:_The_Third_Obama-Romney_Presidential_Debate.txt\n",
        "No collocations found in October_28,_1980:_The_Carter-Reagan_Presidential_Debate.txt\n",
        "No collocations found in October_3,_2000:_The_First_Gore-Bush_Presidential_Debate.txt\n",
        "No collocations found in October_3,_2012:_The_First_Obama-Romney_Presidential_Debate.txt\n",
        "No collocations found in October_5,_1988:_The_Bentsen-Quayle_Vice_Presidential_Debate.txt\n",
        "No collocations found in October_5,_2000:_The_Lieberman-Cheney_Vice_Presidential_Debate.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No collocations found in October_5,_2004:_The_Cheney-Edwards_Vice_Presidential_Debate.txt\n",
        "No collocations found in October_6,_1976:_The_Second_Carter-Ford_Presidential_Debate.txt\n",
        "No collocations found in October_6,_1996:_The_First_Clinton-Dole_Presidential_Debate.txt\n",
        "No collocations found in October_7,_1960:_The_Second_Kennedy-Nixon_Presidential_Debate.txt\n",
        "No collocations found in October_7,_1984:_The_First_Reagan-Mondale_Presidential_Debate.txt\n",
        "No collocations found in October_7,_2008:_The_Second_McCain-Obama_Presidential_Debate.txt\n",
        "No collocations found in October_8,_2004:_The_Second_Bush-Kerry_Presidential_Debate.txt\n",
        "No collocations found in October_9,_1996:_The_Gore-Kemp_Vice_Presidential_Debate.txt\n",
        "No collocations found in Second_half_of_Debate.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No collocations found in September_21,_1980:_The_Anderson-Reagan_Presidential_Debate.txt\n",
        "No collocations found in September_23,_1976:_The_First_Carter-Ford_Presidential_Debate.txt\n",
        "No collocations found in September_25,_1988:_The_First_Bush-Dukakis_Presidential_Debate.txt\n",
        "No collocations found in September_26,_1960:_The_First_Kennedy-Nixon_Presidential_Debate.txt\n",
        "No collocations found in September_26,_2008:_The_First_McCain-Obama_Presidential_Debate.txt\n",
        "No collocations found in September_30,_2004:_The_First_Bush-Kerry_Presidential_Debate.txt\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Well that didn't work, even with a lowered minimum frequency. (There are results when mfreq is lowered to 1, but they're pretty useless)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Gleaning topics from unigrams\n",
      "\n",
      "Let's go back to the unigrams and see if we can use wordnet to abstract the most frequent nouns into concepts. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_hypernyms(synsets):\n",
      "    \"\"\"\n",
      "    Takes a list of synsets (as generated by wn.synsets) and returns a list of all hypernyms. \n",
      "    \"\"\"\n",
      "    hypernyms = set()\n",
      "    for synset in synsets:\n",
      "        for path in synset.hypernym_paths():\n",
      "            hypernyms.update([h for h in path if h != synset])\n",
      "    return hypernyms"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def common_hypernyms(wordforms, min_depth=3):\n",
      "    \"\"\"\n",
      "    Takes a list of wordforms and extracts all hypernyms associated with the wordforms. \n",
      "    Returns a frequency distribution of of the sysnsets extracted. \n",
      "    arguments:\n",
      "        wordforms - Wordforms to be processed\n",
      "        min_depth - A filter to only include synsets of a certain depth.\n",
      "                    Unintuitively, max_depth is used to calculate the depth of a synset. \n",
      "    \"\"\"\n",
      "    hypernyms = []\n",
      "    for l in wordforms:\n",
      "        hset = get_hypernyms(wn.synsets(l, pos=wn.NOUN))\n",
      "        hypernyms.extend(h for h in hset if h.max_depth()>=min_depth)\n",
      "    return nltk.FreqDist(hypernyms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fd_hypernyms(fd, depth=None, min_depth=3, pos=None):\n",
      "    \"\"\"\n",
      "    Takes a frequency distribution and analyzes the hypernyms of the wordforms contained therein. \n",
      "    Returns a weighted \n",
      "    fd - frequency distribution\n",
      "    depth - How far down fd to look\n",
      "    min_depth - A filter to only include synsets of a certain depth.\n",
      "                Unintuitively, max_depth is used to calculate the depth of a synset. \n",
      "    pos - part of speech to limit sysnsets to\n",
      "    \"\"\"\n",
      "    hypernyms = {}\n",
      "    for wf in fd.keys()[0:depth]:\n",
      "        freq = fd.freq(wf)\n",
      "        hset = get_hypernyms(wn.synsets(wf, pos=pos))\n",
      "        for h in hset:\n",
      "            if h.max_depth()>=min_depth:\n",
      "                if h in hypernyms:\n",
      "                    hypernyms[h] += freq\n",
      "                else:\n",
      "                    hypernyms[h] = freq\n",
      "    \n",
      "    hlist = hypernyms.items()\n",
      "    hlist.sort(key=lambda s: s[1], reverse=True)\n",
      "    return hlist\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "debate_concepts = fd_hypernyms(fd_tokens, pos=wn.NOUN, min_depth=7)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[s[0].definition for s in debate_concepts[0:10]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "['a person who rules or guides or inspires others',\n",
        " 'a person who communicates with others',\n",
        " 'someone who negotiates (confers with others in order to reach a settlement)',\n",
        " 'a person who represents others',\n",
        " 'the chief public representative of a country who may also be the head of government',\n",
        " 'a person who is in charge',\n",
        " 'someone who administers a business',\n",
        " 'the leader of a group meeting',\n",
        " 'a person responsible for the administration of a business',\n",
        " 'an administrative unit in government or business']"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def concept_printer(concepts, n=10):\n",
      "    \"Prints first n concepts in concept list generated by fd_hypernyms\"\n",
      "    print \"{:<20} | {:<10} | {}\".format(\"Concept\", \"Noun Freq\", \"Definition\")\n",
      "    print \"====================================================================\"\n",
      "    for s in debate_concepts[0:10]:\n",
      "        print \"{:<20} | {:<10.3%} |  {}\".format(s[0].lemma_names[0], s[1], s[0].definition)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "concept_printer(debate_concepts, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Concept              | Noun Freq  | Definition\n",
        "====================================================================\n",
        "leader               | 7.989%     |  a person who rules or guides or inspires others\n",
        "communicator         | 5.141%     |  a person who communicates with others\n",
        "negotiator           | 4.295%     |  someone who negotiates (confers with others in order to reach a settlement)\n",
        "representative       | 3.958%     |  a person who represents others\n",
        "head_of_state        | 3.909%     |  the chief public representative of a country who may also be the head of government\n",
        "head                 | 3.396%     |  a person who is in charge\n",
        "administrator        | 3.119%     |  someone who administers a business\n",
        "presiding_officer    | 3.073%     |  the leader of a group meeting\n",
        "executive            | 3.050%     |  a person responsible for the administration of a business\n",
        "division             | 2.969%     |  an administrative unit in government or business\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "syn = debate_concepts[2][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "syn.lemma_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "['negotiator', 'negotiant', 'treater']"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brown_concepts = fd_hypernyms(fd_brown, pos=wn.NOUN, min_depth=7)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "concept_printer(brown_concepts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Concept              | Noun Freq  | Definition\n",
        "====================================================================\n",
        "leader               | 7.989%     |  a person who rules or guides or inspires others\n",
        "communicator         | 5.141%     |  a person who communicates with others\n",
        "negotiator           | 4.295%     |  someone who negotiates (confers with others in order to reach a settlement)\n",
        "representative       | 3.958%     |  a person who represents others\n",
        "head_of_state        | 3.909%     |  the chief public representative of a country who may also be the head of government\n",
        "head                 | 3.396%     |  a person who is in charge\n",
        "administrator        | 3.119%     |  someone who administers a business\n",
        "presiding_officer    | 3.073%     |  the leader of a group meeting\n",
        "executive            | 3.050%     |  a person responsible for the administration of a business\n",
        "division             | 2.969%     |  an administrative unit in government or business\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Chunking Noun Phrases\n",
      "\n",
      "I want to try and pull out noun phrases, as I think that is pretty important for my corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_propnoun_fd(sents):\n",
      "    \"\"\"\n",
      "    Finds proper nouns from tagged sentences and returns a frequency distribution of those nouns.\n",
      "    \"\"\"\n",
      "    grammar = r\"\"\"\n",
      "        NPROP: {<NP>+|<NP><IN.*|DT.*><NP>+}\n",
      "    \"\"\"\n",
      "\n",
      "    noun_parser = nltk.RegexpParser(grammar)\n",
      "    \n",
      "    trees = [t for s in sents for t in noun_parser.parse(s).subtrees() if t.node == \"NPROP\"]\n",
      "    fd = nltk.FreqDist([\" \".join([w[0] for w in t]) for t in trees])\n",
      "    return fd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd_debate_np = get_propnoun_fd(sent_tags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 117
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd_view(fd_debate_np)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "America         |847             |8.844%          \n",
        "Congress        |420             |4.386%          \n",
        "Bush            |332             |3.467%          \n",
        "Iraq            |309             |3.226%          \n",
        "John            |261             |2.725%          \n",
        "Iran            |204             |2.130%          \n",
        "Kennedy         |202             |2.109%          \n",
        "Washington      |199             |2.078%          \n",
        "Republican      |190             |1.984%          \n",
        "Carter          |189             |1.973%          \n"
       ]
      }
     ],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd_brown_np = get_propnoun_fd(brown.tagged_sents(categories=['news']))\n",
      "fd_view(fd_brown_np)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "Mr.             |51              |1.139%          \n",
        "U.S.            |44              |0.983%          \n",
        "Kennedy         |41              |0.916%          \n",
        "Mantle          |41              |0.916%          \n",
        "Dallas          |37              |0.827%          \n",
        "Laos            |36              |0.804%          \n",
        "Palmer          |36              |0.804%          \n",
        "Maris           |33              |0.737%          \n",
        "Congo           |30              |0.670%          \n",
        "Washington      |29              |0.648%          \n"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Looking at common verbs and their objects\n",
      "\n",
      "I think it would be interesting to look at common verbs, then use chunking to see what their objects are generally."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "porter = nltk.stem.PorterStemmer()\n",
      "\n",
      "def common_verbs(tags, min_length=4, pos=r\"V.*\"):\n",
      "    \"\"\"Takes a tagset and returns a frequency distribution of the words\n",
      "    that are at least min_length and whose tag matches pos\"\"\"\n",
      "    fd_verbs = nltk.FreqDist([ porter.stem(t[0].lower()) for t in tags if len(t[0]) >= min_length and re.match(pos, t[1])])\n",
      "    return fd_verbs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd_debate_vb = common_verbs(tags)\n",
      "fd_view(fd_debate_vb)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "think           |2,344           |4.052%          \n",
        "go              |2,117           |3.660%          \n",
        "make            |1,700           |2.939%          \n",
        "want            |1,697           |2.934%          \n",
        "said            |1,401           |2.422%          \n",
        "know            |1,375           |2.377%          \n",
        "believ          |993             |1.717%          \n",
        "take            |904             |1.563%          \n",
        "work            |830             |1.435%          \n",
        "unit            |828             |1.431%          \n"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_verb_phrases(sent_tags):\n",
      "    grammar = r\"\"\"\n",
      "        NPROP: {<NP>+|<NP><IN.*|DT.*><NP>+}\n",
      "        VPHRASE: {<V.*><DET>?<NPROP>}\n",
      "    \"\"\"\n",
      "    \n",
      "    parser = nltk.RegexpParser(grammar)\n",
      "    trees = [t for s in sent_tags for t in parser.parse(s).subtrees() if t.node == \"VPHRASE\"]\n",
      "    return trees"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 118
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_verb_subjects_fd(vphrases, verb, stem=True):\n",
      "    \"\"\"\n",
      "    Takes a list of verb phrases, as made by get_verb_phrases and a verb\n",
      "    and returns a frequency distribution of the subjects of that verb. \n",
      "    \"\"\"\n",
      "       \n",
      "    if stem:\n",
      "        verb = porter.stem(verb)\n",
      "    \n",
      "    subjects = []\n",
      "    \n",
      "    for phrase in vphrases:\n",
      "        v = phrase[0][0]\n",
      "        if stem:\n",
      "            v = porter.stem(v)\n",
      "        \n",
      "        if v == verb:\n",
      "            print phrase\n",
      "            subjects.append(\" \".join([w[0] for w in phrase[1]]))\n",
      "    \n",
      "    return nltk.FreqDist(subjects)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vphrases = get_verb_phrases(sent_tags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 158
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_top_verb_subjects(sent_tags, n=10):\n",
      "    \"\"\"\n",
      "    Takes a list of tagged sentences, finds the most common verbs and\n",
      "    creates a frequency distribution for the subjects of the most used verbs. \n",
      "    \n",
      "    n - the number of verbs to test\n",
      "    \"\"\"\n",
      "    \n",
      "    vphrases = get_verb_phrases(sent_tags)\n",
      "    verbs = common_verbs([t for s in sent_tags for t in s])\n",
      "    \n",
      "    v_subjects = {}\n",
      "    \n",
      "    for v in verbs.keys()[0:n]:\n",
      "        v_subjects[v] = get_verb_subjects_fd(vphrases, v)\n",
      "        \n",
      "    return v_subjects"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "debates_vsubjs = get_top_verb_subjects(sent_tags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(VPHRASE think/VB (NPROP Iran/NP))\n",
        "(VPHRASE think/VB (NPROP America/NP))\n",
        "(VPHRASE think/VB (NPROP America/NP))\n",
        "(VPHRASE think/VB (NPROP Ross/NP))\n",
        "(VPHRASE think/VB (NPROP Ross/NP))\n",
        "(VPHRASE think/VB (NPROP America/NP))\n",
        "(VPHRASE think/VB (NPROP America/NP))\n",
        "(VPHRASE think/VB (NPROP George/NP W./NP Bush/NP))\n",
        "(VPHRASE think/VB (NPROP Al/NP Gore/NP))\n",
        "(VPHRASE think/VB (NPROP America/NP))\n",
        "(VPHRASE think/VB (NPROP John/NP))\n",
        "(VPHRASE think/VB (NPROP America/NP))\n",
        "(VPHRASE make/VB (NPROP America/NP))\n",
        "(VPHRASE making/VBG (NPROP Illinois/NP))\n",
        "(VPHRASE make/VB (NPROP America/NP))\n",
        "(VPHRASE make/VB (NPROP America/NP))\n",
        "(VPHRASE make/VB (NPROP America/NP))\n",
        "(VPHRASE make/VB (NPROP John/NP))\n",
        "(VPHRASE make/VB (NPROP America/NP))\n",
        "(VPHRASE make/VB (NPROP America/NP))\n",
        "(VPHRASE make/VB (NPROP America/NP))\n",
        "(VPHRASE making/VBG (NPROP America/NP))\n",
        "(VPHRASE make/VB (NPROP America/NP))\n",
        "(VPHRASE make/VB (NPROP Israel/NP))\n",
        "(VPHRASE make/VB (NPROP America/NP))\n",
        "(VPHRASE make/VB (NPROP America/NP))\n",
        "(VPHRASE make/VB (NPROP America/NP))\n",
        "(VPHRASE make/VB (NPROP America/NP))\n",
        "(VPHRASE make/VB (NPROP America/NP))\n",
        "(VPHRASE make/VB (NPROP America/NP))\n",
        "(VPHRASE want/VB (NPROP America/NP))\n",
        "(VPHRASE want/VB (NPROP Joe/NP))\n",
        "(VPHRASE want/VB (NPROP Roe/NP))\n",
        "(VPHRASE want/VB (NPROP America/NP))\n",
        "(VPHRASE want/VB (NPROP U.S./NP))"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(VPHRASE said/VBD (NPROP Barry/NP Goldwater/NP))\n",
        "(VPHRASE said/VBD (NPROP Bob/NP))\n",
        "(VPHRASE said/VBD (NPROP Jim/NP Baker/NP))\n",
        "(VPHRASE said/VBD (NPROP George/NP))\n",
        "(VPHRASE said/VBD (NPROP Joey/NP))\n",
        "(VPHRASE said/VBD (NPROP Russia/NP))\n",
        "(VPHRASE said/VBD (NPROP Russia/NP))\n",
        "(VPHRASE said/VBN (NPROP Iraq/NP))\n",
        "(VPHRASE said/VBD (NPROP America/NP))\n",
        "(VPHRASE said/VBN (NPROP America/NP))\n",
        "(VPHRASE said/VBD (NPROP Bob/NP))\n",
        "(VPHRASE know/VB (NPROP Barbara/NP))\n",
        "(VPHRASE know/VB (NPROP Al/NP))\n",
        "(VPHRASE know/VB (NPROP Sarah/NP))\n",
        "(VPHRASE know/VB (NPROP America/NP))\n",
        "(VPHRASE know/VB (NPROP Gwen/NP))\n",
        "(VPHRASE know/VB (NPROP Donald/NP))\n",
        "(VPHRASE knows/VBZ (NPROP Missouri/NP))\n",
        "(VPHRASE believe/VB (NPROP Washington/NP))\n",
        "(VPHRASE believe/VB (NPROP Bill/NP Clinton/NP))\n",
        "(VPHRASE believe/VB (NPROP America/NP))\n",
        "(VPHRASE believe/VB (NPROP Roe/NP))\n",
        "(VPHRASE believe/VB (NPROP John/NP))\n",
        "(VPHRASE believe/VB (NPROP John/NP))\n",
        "(VPHRASE believe/VB (NPROP Castro/NP))\n",
        "(VPHRASE believe/VB (NPROP China/NP))\n",
        "(VPHRASE believe/VB (NPROP America/NP))\n",
        "(VPHRASE believe/VB (NPROP America/NP))\n",
        "(VPHRASE believe/VB (NPROP America/NP))\n",
        "(VPHRASE believe/VB (NPROP America/NP))\n",
        "(VPHRASE take/VB (NPROP Russia/NP))\n",
        "(VPHRASE taking/VBG (NPROP Formosa/NP))\n",
        "(VPHRASE take/VB (NPROP Joe/NP))\n",
        "(VPHRASE take/VB (NPROP Joe/NP))\n",
        "(VPHRASE take/VB (NPROP California/NP))\n",
        "(VPHRASE take/VB (NPROP Detroit/NP))\n",
        "(VPHRASE take/VB (NPROP Detroit/NP))\n",
        "(VPHRASE take/VB (NPROP America/NP))\n",
        "(VPHRASE take/VB (NPROP Israel/NP))"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(VPHRASE take/VB (NPROP America/NP))\n",
        "(VPHRASE take/VB (NPROP America/NP))\n"
       ]
      }
     ],
     "prompt_number": 159
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in debates_vsubjs.keys():\n",
      "    print k\n",
      "    fd_view(debates_vsubjs[k])\n",
      "    print \"********************************************\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "said\n",
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "America         |2               |18.182%         \n",
        "Bob             |2               |18.182%         \n",
        "Russia          |2               |18.182%         \n",
        "Barry Goldwater |1               |9.091%          \n",
        "George          |1               |9.091%          \n",
        "Iraq            |1               |9.091%          \n",
        "Jim Baker       |1               |9.091%          \n",
        "Joey            |1               |9.091%          \n",
        "********************************************\n",
        "make\n",
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "America         |15              |83.333%         \n",
        "Illinois        |1               |5.556%          \n",
        "Israel          |1               |5.556%          \n",
        "John            |1               |5.556%          \n",
        "********************************************\n",
        "work\n",
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "********************************************\n",
        "know\n",
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "Al              |1               |14.286%         \n",
        "America         |1               |14.286%         \n",
        "Barbara         |1               |14.286%         \n",
        "Donald          |1               |14.286%         \n",
        "Gwen            |1               |14.286%         \n",
        "Missouri        |1               |14.286%         \n",
        "Sarah           |1               |14.286%         \n",
        "********************************************\n",
        "want\n",
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "America         |2               |40.000%         \n",
        "Joe             |1               |20.000%         \n",
        "Roe             |1               |20.000%         \n",
        "U.S.            |1               |20.000%         \n",
        "********************************************\n",
        "go\n",
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "********************************************\n",
        "take\n",
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "America         |3               |27.273%         \n",
        "Detroit         |2               |18.182%         \n",
        "Joe             |2               |18.182%         \n",
        "California      |1               |9.091%          \n",
        "Formosa         |1               |9.091%          \n",
        "Israel          |1               |9.091%          \n",
        "Russia          |1               |9.091%          \n",
        "********************************************\n",
        "think\n",
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "America         |6               |50.000%         \n",
        "Ross            |2               |16.667%         \n",
        "Al Gore         |1               |8.333%          \n",
        "George W. Bush  |1               |8.333%          \n",
        "Iran            |1               |8.333%          \n",
        "John            |1               |8.333%          \n",
        "********************************************\n",
        "unit\n",
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "********************************************\n",
        "believ\n",
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "America         |5               |41.667%         \n",
        "John            |2               |16.667%         \n",
        "Bill Clinton    |1               |8.333%          \n",
        "Castro          |1               |8.333%          \n",
        "China           |1               |8.333%          \n",
        "Roe             |1               |8.333%          \n",
        "Washington      |1               |8.333%          \n",
        "********************************************\n"
       ]
      }
     ],
     "prompt_number": 156
    }
   ],
   "metadata": {}
  }
 ]
}