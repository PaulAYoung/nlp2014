{
 "metadata": {
  "name": "",
  "signature": "sha256:0907898d1f389883d99675c3400ddd997fc7626943d68b024ec422071fb9dab2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1. Syntactic Patterns for Technical Terms ##"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.corpus import brown"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As seen in the Chuang et al. paper and in the Manning and Schuetze chapter,\n",
      "there is a well-known part-of-speech based pattern defined by Justeson and Katz\n",
      "for identifying simple noun phrases that often words well for pulling out keyphrases.\n",
      "\n",
      "Chuang et al use this pattern: Technical Term  T = (A | N)+ (N | C)  | N\n",
      "\n",
      "Below, please write a function to  define a chunker using the RegexpParser as illustrated in the section *Chunking with Regular Expressions*.  You'll need to revise the grammar rules shown there to match the pattern shown above.  You can be liberal with your definition of what is meant by *N* here.  Also, C refers to cardinal number, which is CD in the brown corpus.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grammar = r\"\"\"\n",
      "    T: {<JJ.*|N.*>+ <N.*|CC|CS>|<N.*>}\n",
      "\"\"\"\n",
      "\n",
      "t_term = nltk.RegexpParser(grammar)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.help.brown_tagset(\"CD.*\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CD: numeral, cardinal\n",
        "    two one 1 four 2 1913 71 74 637 1937 8 five three million 87-31 29-5\n",
        "    seven 1,119 fifty-three 7.5 billion hundred 125,000 1,700 60 100 six\n",
        "    ...\n",
        "CD$: numeral, cardinal, genitive\n",
        "    1960's 1961's .404's\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below, please write a function to call the chunker, run it on some sentences, and then print out the results for  those sentences.\n",
      "\n",
      "For uniformity, please run it on sentences 100 through 104 from the tagged brown corpus news category.\n",
      "\n",
      "Then extract out the phrases themselves using the subtree extraction technique shown in the \n",
      "*Exploring Text Corpora* category.  (Note: Section 7.4 shows how to get to the actual words in the phrase by using the tree.leaves() command.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sents = brown.tagged_sents()[100:105]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parsed_sents = [t_term.parse(s) for s in sents]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tech_phrases = [[t for t in s.subtrees() if t.node==\"T\"] for s in parsed_sents]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tech_phrases"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[[Tree('T', [('Daniel', 'NP')]),\n",
        "  Tree('T', [('fight', 'NN')]),\n",
        "  Tree('T', [('measure', 'NN')]),\n",
        "  Tree('T', [('rejection', 'NN')]),\n",
        "  Tree('T', [('previous', 'JJ'), ('Legislatures', 'NNS-TL')]),\n",
        "  Tree('T', [('public', 'JJ'), ('hearing', 'NN')]),\n",
        "  Tree('T', [('House', 'NN-TL'), ('Committee', 'NN-TL')]),\n",
        "  Tree('T', [('Revenue', 'NN-TL')]),\n",
        "  Tree('T', [('Taxation', 'NN-TL')])],\n",
        " [Tree('T', [('committee', 'NN'), ('rules', 'NNS')]),\n",
        "  Tree('T', [('subcommittee', 'NN')]),\n",
        "  Tree('T', [('week', 'NN')])],\n",
        " [Tree('T', [('questions', 'NNS')]),\n",
        "  Tree('T', [('committee', 'NN'), ('members', 'NNS')]),\n",
        "  Tree('T', [('bankers', 'NNS')]),\n",
        "  Tree('T', [('witnesses', 'NNS')]),\n",
        "  Tree('T', [('doubt', 'NN'), ('that', 'CS')]),\n",
        "  Tree('T', [('passage', 'NN')])],\n",
        " [Tree('T', [('Daniel', 'NP')]),\n",
        "  Tree('T', [('estimate', 'NN'), ('that', 'CS')]),\n",
        "  Tree('T', [('dollars', 'NNS')]),\n",
        "  Tree('T', [('deficit', 'NN')]),\n",
        "  Tree('T', [('dollars', 'NNS')]),\n",
        "  Tree('T', [('end', 'NN')]),\n",
        "  Tree('T', [('current', 'JJ'), ('fiscal', 'JJ'), ('year', 'NN')]),\n",
        "  Tree('T', [('Aug.', 'NP')])],\n",
        " [Tree('T', [('committee', 'NN')]),\n",
        "  Tree('T', [('measure', 'NN')]),\n",
        "  Tree('T', [('means', 'NNS')]),\n",
        "  Tree('T', [('escheat', 'NN'), ('law', 'NN')]),\n",
        "  Tree('T', [('books', 'NNS')]),\n",
        "  Tree('T', [('Texas', 'NP')]),\n",
        "  Tree('T', [('republic', 'NN')])]]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2. Identify Proper Nouns ##\n",
      "For this next task, write a new version of the chunker, but this time change it in two ways:\n",
      " 1. Make it recognize proper nouns\n",
      " 2. Make it work on your personal text collection\u00a0which means that you need to run a tagger over your personal text collection.\n",
      "\n",
      "Note that the second requirements means that you need to run a tagger over your personal text collection before you design the proper noun recognizer.  You can use a pre-trained tagger or train your own on one of the existing tagged collections (brown, conll, or treebank)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Tagger:** Your code for optionally training tagger, and for definitely running tagger on your personal collection goes here:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import corpii\n",
      "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "debates= nltk.clean_html(corpii.load_pres_debates().raw())\n",
      "sents = sent_tokenizer.sentences_from_text(debates)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_backoff_tagger (train_sents):\n",
      "    t0 = nltk.DefaultTagger('NN')\n",
      "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
      "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
      "    return t2\n",
      "\n",
      "tagger = build_backoff_tagger(brown.tagged_sents())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "token_regex= \"\"\"(?x)\n",
      "    # taken from ntlk book example\n",
      "    ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
      "    | \\w+(-\\w+)*        # words with optional internal hyphens\n",
      "    | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
      "    | \\.\\.\\.            # ellipsis\n",
      "    | [][.,;\"'?():-_`]  # these are separate tokens\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t_sents = [nltk.regexp_tokenize(s, token_regex) for s in sents]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tagged_sents = [tagger.tag(s) for s in t_sents]\n",
      "#tags = tagger.tag(tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Chunker:** Code for the proper noun chunker goes here:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re_noun_chunk = r\"\"\"\n",
      "    NP: {<NP>+}\n",
      "\"\"\"\n",
      "\n",
      "np_parser = nltk.RegexpParser(re_noun_chunk)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Test the Chunker:** Test your proper noun recognizer on a lot of sentences to see how well it is working.  You might want to add prepositions in order to improve your results.  \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(0,50):\n",
      "    print \"**********************************************\"\n",
      "    print sents[i]\n",
      "    print \"Proper Nouns:\"\n",
      "    print [t for t in np_parser.parse(tagged_sents[i]).subtrees() if t.node==\"NP\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "**********************************************\n",
        "October 15, 1992 First Half Debate Transcript \n",
        " \n",
        " October 15, 1992 \n",
        " The Second Clinton-Bush-Perot Presidential Debate (First Half of Debate) \n",
        " This is the first half of the transcript of the Richmond debate.\n",
        "Proper Nouns:\n",
        "[Tree('NP', [('October', 'NP')]), Tree('NP', [('October', 'NP')])]\n",
        "**********************************************\n",
        "The October 15th \"town hall\" format debate was moderated by Carole Simpson.\n",
        "Proper Nouns:\n",
        "[Tree('NP', [('October', 'NP')]), Tree('NP', [('Simpson', 'NP')])]\n",
        "**********************************************\n",
        "She explains the format in her opening remarks.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "The length of this printed transcript is approximately 20 pages.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "CAROLE SIMPSON: Good evening and welcome to this second of three presidential debates between the major candidates for president of the US.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "The candidates are the Republican nominee, President George Bush, the independent Ross Perot and Governor Bill Clinton, the Democratic nominee.\n",
        "Proper Nouns:\n",
        "[Tree('NP', [('Republican', 'NP')]), Tree('NP', [('George', 'NP'), ('Bush', 'NP')]), Tree('NP', [('Ross', 'NP')]), Tree('NP', [('Bill', 'NP'), ('Clinton', 'NP')])]\n",
        "**********************************************\n",
        "My name is Carole Simpson, and I will be the moderator for tonight's 90-minute debate, which is coming to you from the campus of the University of Richmond in Richmond, Virginia.\n",
        "Proper Nouns:\n",
        "[Tree('NP', [('Simpson', 'NP')]), Tree('NP', [('Richmond', 'NP')]), Tree('NP', [('Richmond', 'NP')]), Tree('NP', [('Virginia', 'NP')])]\n",
        "**********************************************\n",
        "Now, tonight's program is unlike any other presidential debate in history.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "We're making history now and it's pretty exciting.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "An independent polling firm has selected an audience of 209 uncommitted voters from this area.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "The candidates will be asked questions by these voters on a topic of their choosing -- anything they want to ask about.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "My job as moderator is to, you know, take care of the questioning, ask questions myself if I think there needs to be continuity and balance, and sometimes I might ask the candidates to respond to what another candidate may have said.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "Now, the format has been agreed to by representatives of both the Republican and Democratic campaigns, and there is no subject matter that is restricted.\n",
        "Proper Nouns:\n",
        "[Tree('NP', [('Republican', 'NP')])]\n",
        "**********************************************\n",
        "Anything goes.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "We can ask anything.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "After the debate, the candidates will have an opportunity to make a closing statement.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "So, President Bush, I think you said it earlier -- let's get it on.\n",
        "Proper Nouns:\n",
        "[Tree('NP', [('Bush', 'NP')])]\n",
        "**********************************************\n",
        "PRESIDENT GEORGE BUSH: Let's go.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "SIMPSON: And I think the first question is over here.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "AUDIENCE QUESTION: Yes.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "I'd like to direct my question to Mr. Perot.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "What will you do as president to open foreign markets to fair competition from American business and to stop unfair competition here at home from foreign countries so that we can bring jobs back to the US?\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "ROSS PEROT: That's right at the top of my agenda.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "We've shipped millions of jobs overseas and we have a strange situation because we have a process in Washington where after you've served for a while you cash in, become a foreign lobbyist, make $30,000 a month, then take a leave, work on presidential campaigns, make sure you've got good contacts and then go back out.\n",
        "Proper Nouns:\n",
        "[Tree('NP', [('Washington', 'NP')])]\n",
        "**********************************************\n",
        "Now, if you just want to get down to brass tacks, first thing you ought to do is get all these folks who've got these 1-way trade agreements that we've negotiated over the years and say fellas, we'll take the same deal we gave you.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "And they'll gridlock right at that point because for example, we've got international competitors who simply could not unload their cars off the ships if they had to comply -- you see, if it was a 2-way street, just couldn't do it.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "We have got to stop sending jobs overseas.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "To those of you in the audience who are business people: pretty simple.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "If you're paying $12, $13, $14 an hour for a factory worker, and you can move your factory south of the border, pay $1 an hour for labor, hire a young -- let's assume you've been in business for a long time.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "You've got a mature workforce.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "Pay $1 an hour for your labor, have no health care -- that's the most expensive single element in making the car.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "Have no environmental controls, no pollution controls and no retirement.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "And you don't care about anything but making money.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "There will be a job-sucking sound going south.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "If the people send me to Washington the first thing I'll do is study that 2000-page agreement and make sure it's a 2-way street.\n",
        "Proper Nouns:\n",
        "[Tree('NP', [('Washington', 'NP')])]\n",
        "**********************************************\n",
        "One last point here.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "I decided I was dumb and didn't understand it so I called a \"Who's Who\" of the folks that have been around it, and I said why won't everybody go south; they said it will be disruptive; I said for how long.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "I finally got 'em for 12 to 15 years.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "And I said, well, how does it stop being disruptive?\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "And that is when their jobs come up from a dollar an hour to $6 an hour, and ours go down to $6 an hour; then it's leveled again, but in the meantime you've wrecked the country with these kind of deals.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "We got to cut it out.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "SIMPSON: Thank you, Mr. Perot.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "I see that the president has stood up, so he must have something to say about this.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "BUSH: Carole, the thing that saved us in this global economic slowdown has been our exports, and what I'm trying to do is increase our exports.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "And if indeed all the jobs were going to move south because there are lower wages, there are lower wages now and they haven't done that.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "And so I have just negotiated with the president of Mexico the North American Free Trade Agreement -- and the prime minister of Canada, I might add -- and I want to have more of these free trade agreements, because export jobs are increasing far faster than any jobs that may have moved overseas.\n",
        "Proper Nouns:\n",
        "[Tree('NP', [('Mexico', 'NP')]), Tree('NP', [('Canada', 'NP')])]\n",
        "**********************************************\n",
        "That's a scare tactic, because it's not that many.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "But any one that's here, we want to have more jobs here.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "And the way to do that is to increase our exports.\n",
        "Proper Nouns:\n",
        "[]\n",
        "**********************************************\n",
        "Some believe in protection.\n",
        "Proper Nouns:\n",
        "[]\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Notes\n",
      "\n",
      "I tried adding prepositions and it didn't really help. There is still more improvement that can be done at the tagging stage. For example, tokens following salutations like \"Mr. Perot\" are not being seen as proper nouns by the tokenizer. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**FreqDist Results:** After you have your proper noun recognizer working to your satisfaction, below  run it over your entire collection, feed the results into a FreqDist, and then print out the top 20 proper nouns by frequency.  That code goes here, along with the output:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trees = [np_parser.parse(s) for s in tagged_sents]\n",
      "pnouns = [i for t in trees for i in t.subtrees() if i.node==\"NP\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pn_freq = nltk.FreqDist([pn.pprint() for pn in pnouns])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pn_freq.items()[0:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "[('(NP America/NP)', 847),\n",
        " ('(NP Congress/NP)', 420),\n",
        " ('(NP Bush/NP)', 332),\n",
        " ('(NP Iraq/NP)', 309),\n",
        " ('(NP John/NP)', 261),\n",
        " ('(NP Iran/NP)', 204),\n",
        " ('(NP Kennedy/NP)', 202),\n",
        " ('(NP Washington/NP)', 199),\n",
        " ('(NP Republican/NP)', 190),\n",
        " ('(NP Carter/NP)', 189),\n",
        " ('(NP Ford/NP)', 172),\n",
        " ('(NP Jim/NP)', 139),\n",
        " ('(NP Clinton/NP)', 137),\n",
        " ('(NP Israel/NP)', 129),\n",
        " ('(NP Nixon/NP)', 126),\n",
        " ('(NP China/NP)', 123),\n",
        " ('(NP Bob/NP)', 121),\n",
        " ('(NP George/NP Bush/NP)', 111),\n",
        " ('(NP Gore/NP)', 109),\n",
        " ('(NP Bill/NP Clinton/NP)', 99)]"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### For Wednesday ###\n",
      "Just FYI, in Wednesday's October 8's assignment, you'll be asked to extend this code a bit more to discover interesting patterns using objects or subjects of verbs, and do a bit of Wordnet grouping.  This will be posted soon.  Note that these exercises are intended to provide you with functions to use directly in your larger assignment. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    }
   ],
   "metadata": {}
  }
 ]
}