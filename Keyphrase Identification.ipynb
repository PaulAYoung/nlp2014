{
 "metadata": {
  "gist_id": "18c23282dd08e6579dc5",
  "name": "",
  "signature": "sha256:97dd49d8bac30fe83ee973d323db185297c5d7b2351d4583e2e5df78b2241db0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Keyphrase Identification\n",
      "\n",
      "This notebook will try several methods for identifying keyphrases in text. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Overview\n",
      "\n",
      "###Algorithms\n",
      "\n",
      "The three algorithms I am presenting are:\n",
      "    1. Most common nouns - A simple look at noun unigrams to find the most common. \n",
      "    2. Most used concepts - Uses WordNet to identify common concepts from the most common nouns. \n",
      "    3. Proper noun extraction - Extracts proper nouns from text. \n",
      "    \n",
      "There is more description of these algorithms below. \n",
      "\n",
      "My mystery.txt file comes from the most used concepts algorithm. \n",
      "\n",
      "###Other Experiments\n",
      "\n",
      "I tried several other things including\n",
      "    1. Collocations - I tried both PMI and chi squared measures. While some interesting entities were extracted, they did not provide much insight onto the text. \n",
      "    2. Common verbs and their subjects - I tried finding the most common verbs and then used chunking to find the most common subjects of the most common verbs. This produced very confusing results. I think this was a case of a more complex algorithm not working very well. \n",
      "    3. General noun phrase extraction - Using chunking to find common noun phrases. This was useful but surprisingly not much better than the simple unigram noun extraction. I decided to include the unigram method because it is foundational for the concept extraction method, which was my favorite. \n",
      "\n",
      "###Guess on Mystery Text\n",
      "\n",
      "I haven't actually opened the mystery text file, so I thought it would be fun to take a guess at the contents based on my algorithms:\n",
      "\n",
      "I'm certain it has to do with international commerce. I am going to guess that it's further narrowed to agriculture. \n",
      "\n",
      "After taking a quick look at mystery.txt: Not bad! It's not completely focused on agriculture but there are a lot of articles about it. I want to figure out why other issues discussed in the corpus, such as fuel and labor, were missed. \n",
      "\n",
      "I just checked in wordnet and all of the synsets from fuel have a max depth of 3 or 4. I filtered concepts to have a depth of at least 7, to avoid having very broad concepts like organism jump in. A more intelligent way of doing this is something I'll be thinking about. \n",
      "\n",
      "###Other thoughts\n",
      "\n",
      "I had a tough time interpreting results for the brown news corpus. I'm hoping it's because news is very broad, so there are no clear categories to be found but I look forward to seeing if other students had algorithms that were more fruitful there. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Setup\n",
      "\n",
      "First, let's import modules, load text, and do the required tokenizing and tagging. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "from os import path\n",
      "\n",
      "import nltk\n",
      "from nltk.corpus import brown\n",
      "from nltk.corpus import wordnet as wn\n",
      "\n",
      "import corpii"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from urllib import urlopen"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can uncomment some lines below to use a different text source. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = nltk.clean_html(corpii.load_pres_debates().raw())\n",
      "\n",
      "# code to get text from alternate source\n",
      "#text = urlopen(\"www.url.com\").read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's tokenize, split into sentences, and tag the text. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tokenize\n",
      "token_regex= \"\"\"(?x)\n",
      "    # taken from ntlk book example\n",
      "    ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
      "    | \\w+(-\\w+)*        # words with optional internal hyphens\n",
      "    | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
      "    | \\.\\.\\.            # ellipsis\n",
      "    | [][.,;\"'?():-_`]  # these are separate tokens\n",
      "\"\"\"\n",
      "\n",
      "tokens = nltk.regexp_tokenize(text, token_regex)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# we're going to use frequency distributions a lot, so let's create a nice way of looking at those\n",
      "DISPLAY_LIM = 25\n",
      "\n",
      "def fd_view(fd, n=DISPLAY_LIM):\n",
      "    \"\"\"Prints a nice format of items in FreqDist fd[0:n]\"\"\"\n",
      "    print \"{:<16}|{:<16}|{:<16}\".format(\"Word\", \"Count\", \"Frequency\")\n",
      "    print \"=========================================================\"\n",
      "    for i in fd.items()[0:n]:\n",
      "        print \"{:<16}|{:<16,d}|{:<16.3%}\".format(i[0], i[1], fd.freq(i[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get sentences\n",
      "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "sents = list(sent_tokenizer.sentences_from_tokens(tokens))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Create tagger\n",
      "\n",
      "def build_backoff_tagger (train_sents):\n",
      "    t0 = nltk.DefaultTagger('NN')\n",
      "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
      "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
      "    return t2\n",
      "\n",
      "tagger = build_backoff_tagger(brown.tagged_sents())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "debate_sents = [tagger.tag(s) for s in sents]\n",
      "debate_tags = [t for s in debate_sents for t in s]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brown_sents = brown.tagged_sents(categories=['news'])\n",
      "brown_tags = brown.tagged_words(categories=['news'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myst_file = open(path.join(\"text\", \"mystery.txt\"), \"r\")\n",
      "myst_text = myst_file.read()\n",
      "myst_file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myst_tokens = nltk.word_tokenize(myst_text)\n",
      "myst_sents = [nltk.tag.pos_tag(s) for s in sent_tokenizer.sentences_from_tokens(myst_tokens)]\n",
      "myst_tags = [t for s in myst_sents for t in s]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1 - Simple unigram approach\n",
      "\n",
      "To start let's try just finding the most common nouns in the corpus and see how that does. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def common_nouns(tags, min_length=4, pos=r\"N.*\"):\n",
      "    \"\"\"Takes a tagset and returns a frequency distribution of the words\n",
      "    that are at least min_length and whose tag matches pos\"\"\"\n",
      "    fd_nouns = nltk.FreqDist([ t[0].lower() for t in tags if len(t[0]) >= min_length and re.match(pos, t[1])])\n",
      "    return fd_nouns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "debate_fd_nouns = common_nouns(debate_tags)\n",
      "fd_view(debate_fd_nouns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "president       |3,052           |2.409%          \n",
        "people          |2,601           |2.053%          \n",
        "senator         |1,265           |0.998%          \n",
        "years           |1,202           |0.949%          \n",
        "country         |1,173           |0.926%          \n",
        "question        |1,153           |0.910%          \n",
        "time            |1,046           |0.826%          \n",
        "governor        |1,028           |0.811%          \n",
        "america         |1,025           |0.809%          \n",
        "bush            |937             |0.740%          \n",
        "states          |886             |0.699%          \n",
        "government      |856             |0.676%          \n",
        "world           |849             |0.670%          \n",
        "administration  |659             |0.520%          \n",
        "plan            |651             |0.514%          \n",
        "obama           |648             |0.511%          \n",
        "jobs            |618             |0.488%          \n",
        "year            |600             |0.474%          \n",
        "security        |595             |0.470%          \n",
        "money           |576             |0.455%          \n",
        "things          |573             |0.452%          \n",
        "percent         |571             |0.451%          \n",
        "health          |552             |0.436%          \n",
        "care            |532             |0.420%          \n",
        "lehrer          |507             |0.400%          \n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brown_fd_nouns = common_nouns(brown_tags)\n",
      "fd_view(brown_fd_nouns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "mrs.            |253             |0.899%          \n",
        "state           |151             |0.537%          \n",
        "president       |142             |0.505%          \n",
        "year            |142             |0.505%          \n",
        "home            |132             |0.469%          \n",
        "time            |103             |0.366%          \n",
        "years           |102             |0.362%          \n",
        "house           |96              |0.341%          \n",
        "week            |94              |0.334%          \n",
        "city            |93              |0.330%          \n",
        "school          |87              |0.309%          \n",
        "committee       |75              |0.266%          \n",
        "members         |74              |0.263%          \n",
        "government      |73              |0.259%          \n",
        "university      |70              |0.249%          \n",
        "bill            |69              |0.245%          \n",
        "kennedy         |66              |0.235%          \n",
        "john            |65              |0.231%          \n",
        "night           |65              |0.231%          \n",
        "program         |65              |0.231%          \n",
        "board           |64              |0.227%          \n",
        "administration  |62              |0.220%          \n",
        "county          |61              |0.217%          \n",
        "states          |60              |0.213%          \n",
        "meeting         |58              |0.206%          \n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myst_fd_nouns = common_nouns(myst_tags)\n",
      "fd_view(myst_fd_nouns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "said.           |606             |2.244%          \n",
        "tonnes          |472             |1.748%          \n",
        "u.s.            |425             |1.573%          \n",
        "dlrs            |320             |1.185%          \n",
        "dollar          |283             |1.048%          \n",
        "trade           |262             |0.970%          \n",
        "wheat           |235             |0.870%          \n",
        "japan           |231             |0.855%          \n",
        "market          |197             |0.729%          \n",
        "prices          |196             |0.726%          \n",
        "year            |191             |0.707%          \n",
        "coffee          |188             |0.696%          \n",
        "bank            |181             |0.670%          \n",
        "week            |159             |0.589%          \n",
        "export          |142             |0.526%          \n",
        "exports         |134             |0.496%          \n",
        "gold            |132             |0.489%          \n",
        "price           |132             |0.489%          \n",
        "rice            |129             |0.478%          \n",
        "stocks          |123             |0.455%          \n",
        "grain           |115             |0.426%          \n",
        "production      |114             |0.422%          \n",
        "april           |111             |0.411%          \n",
        "government      |111             |0.411%          \n",
        "department      |109             |0.404%          \n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Analysis\n",
      "\n",
      "The simple approach does pretty well. For the debates, many of the top nouns have to do with governance. The Brown news corpus is a bit more scattered, although news is a pretty broad category itself. I would say it does a decent job with mystery text and from these results would guess the mystery text has something to do with trade, "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##2 - Gleaning topics from unigrams\n",
      "\n",
      "\n",
      "My next most succesffull experiment expanded on the unigram approach to find common concepts from the top nouns. Basically, I take the most common nouns and look at each of their hypernym paths and to determine how often concepts are referred to in a text. This is filtered to only include hypernyms at a certain depth in WordNet's tree (after some experimenting I settled on a depth of 7). \n",
      "\n",
      "I also played around with using least common hypernym for this algorithm but I got some weird results, like the lowest hypernym of president and senator being organism when they both had leader in their hypernym paths. Also, the way I was thinking of using that algorithm had O(n)=n^2, which I wasn't excited about. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_hypernyms(synsets):\n",
      "    \"\"\"\n",
      "    Takes a list of synsets (as generated by wn.synsets) and returns a list of all hypernyms. \n",
      "    \"\"\"\n",
      "    hypernyms = set()\n",
      "    for synset in synsets:\n",
      "        for path in synset.hypernym_paths():\n",
      "            hypernyms.update([h for h in path if h != synset])\n",
      "    return hypernyms"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fd_hypernyms(fd, depth=None, min_depth=7, pos=None):\n",
      "    \"\"\"\n",
      "    Takes a frequency distribution and analyzes the hypernyms of the wordforms contained therein. \n",
      "    Returns a weighted \n",
      "    fd - frequency distribution\n",
      "    depth - How far down fd to look\n",
      "    min_depth - A filter to only include synsets of a certain depth.\n",
      "                Unintuitively, max_depth is used to calculate the depth of a synset. \n",
      "    pos - part of speech to limit sysnsets to\n",
      "    \"\"\"\n",
      "    hypernyms = {}\n",
      "    for wf in fd.keys()[0:depth]:\n",
      "        freq = fd.freq(wf)\n",
      "        hset = get_hypernyms(wn.synsets(wf, pos=pos))\n",
      "        for h in hset:\n",
      "            if h.max_depth()>=min_depth:\n",
      "                if h in hypernyms:\n",
      "                    hypernyms[h] += freq\n",
      "                else:\n",
      "                    hypernyms[h] = freq\n",
      "    \n",
      "    hlist = hypernyms.items()\n",
      "    hlist.sort(key=lambda s: s[1], reverse=True)\n",
      "    return hlist\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def concept_printer(concepts, n=DISPLAY_LIM):\n",
      "    \"Prints first n concepts in concept list generated by fd_hypernyms\"\n",
      "    print \"{:<20} | {:<10} | {}\".format(\"Concept\", \"Concept Freq\", \"Definition\")\n",
      "    print \"====================================================================\"\n",
      "    for s in concepts[0:n]:\n",
      "        print \"{:<20} | {:<12.3%} |  {}\".format(s[0].lemma_names[0], s[1], s[0].definition)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "debate_concepts = fd_hypernyms(debate_fd_nouns, pos=wn.NOUN, min_depth=7)\n",
      "concept_printer(debate_concepts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Concept              | Concept Freq | Definition\n",
        "====================================================================\n",
        "leader               | 8.538%       |  a person who rules or guides or inspires others\n",
        "communicator         | 6.751%       |  a person who communicates with others\n",
        "negotiator           | 5.820%       |  someone who negotiates (confers with others in order to reach a settlement)\n",
        "representative       | 5.516%       |  a person who represents others\n",
        "head_of_state        | 5.472%       |  the chief public representative of a country who may also be the head of government\n",
        "head                 | 3.204%       |  a person who is in charge\n",
        "administrator        | 2.954%       |  someone who administers a business\n",
        "executive            | 2.889%       |  a person responsible for the administration of a business\n",
        "presiding_officer    | 2.770%       |  the leader of a group meeting\n",
        "country              | 2.684%       |  the territory occupied by a nation\n",
        "division             | 2.677%       |  an administrative unit in government or business\n",
        "worker               | 2.602%       |  a person who works at a specific occupation\n",
        "position             | 2.527%       |  a job in an organization\n",
        "department           | 2.452%       |  a specialized division of a large organization\n",
        "corporate_executive  | 2.451%       |  an executive in a business corporation\n",
        "change_of_state      | 2.448%       |  the act of changing something into something different in essential characteristics\n",
        "academic_administrator | 2.447%       |  an administrator in a college or university\n",
        "presidency           | 2.444%       |  the office and function of president\n",
        "President_of_the_United_States | 2.425%       |  the person who holds the office of head of state of the United States government\n",
        "family               | 2.309%       |  people descended from a common ancestor\n",
        "politician           | 2.293%       |  a leader engaged in civil administration\n",
        "curiosity            | 2.292%       |  a state in which you want to learn more about something\n",
        "interest             | 2.053%       |  a sense of concern with and curiosity about someone or something\n",
        "concern              | 2.008%       |  something that interests you because it is important or affects you\n",
        "government_department | 1.950%       |  a department of government\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brown_concepts = fd_hypernyms(brown_fd_nouns, pos=wn.NOUN, min_depth=7)\n",
      "concept_printer(brown_concepts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Concept              | Concept Freq | Definition\n",
        "====================================================================\n",
        "communicator         | 5.010%       |  a person who communicates with others\n",
        "leader               | 4.985%       |  a person who rules or guides or inspires others\n",
        "worker               | 3.820%       |  a person who works at a specific occupation\n",
        "skilled_worker       | 3.006%       |  a worker who has acquired special skills\n",
        "municipality         | 2.935%       |  an urban district having corporate status and powers of self-government\n",
        "adult                | 2.761%       |  a fully developed person from maturity onward\n",
        "writer               | 2.512%       |  writes (books or stories or articles or the like) professionally (for pay)\n",
        "contestant           | 2.405%       |  a person who participates in competitions\n",
        "creator              | 2.398%       |  a person who grows or makes or invents things\n",
        "negotiator           | 2.388%       |  someone who negotiates (confers with others in order to reach a settlement)\n",
        "representative       | 2.331%       |  a person who represents others\n",
        "entertainer          | 2.302%       |  a person who tries to please or amuse\n",
        "performer            | 2.260%       |  an entertainer who performs a dramatic or musical work for an audience\n",
        "city                 | 2.196%       |  a large and densely populated urban area; may include several independent administrative districts\n",
        "head_of_state        | 2.150%       |  the chief public representative of a country who may also be the head of government\n",
        "motion               | 2.075%       |  the act of changing location from one place to another\n",
        "player               | 1.762%       |  a person who participates in or is skilled at some game\n",
        "change_of_state      | 1.691%       |  the act of changing something into something different in essential characteristics\n",
        "relative             | 1.670%       |  a person related by blood or marriage\n",
        "division             | 1.631%       |  an administrative unit in government or business\n",
        "commerce             | 1.528%       |  transactions (sales and purchases) having the objective of supplying commodities (goods and services)\n",
        "scientist            | 1.425%       |  a person with advanced knowledge of one or more sciences\n",
        "athlete              | 1.414%       |  a person trained to compete in sports\n",
        "department           | 1.400%       |  a specialized division of a large organization\n",
        "sports_equipment     | 1.322%       |  equipment needed to participate in a particular sport\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myst_concepts = fd_hypernyms(myst_fd_nouns, pos=wn.NOUN, min_depth=7)\n",
      "concept_printer(myst_concepts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Concept              | Concept Freq | Definition\n",
        "====================================================================\n",
        "country              | 5.131%       |  the territory occupied by a nation\n",
        "commerce             | 5.013%       |  transactions (sales and purchases) having the objective of supplying commodities (goods and services)\n",
        "vascular_plant       | 4.532%       |  green plant having a vascular system: ferns, gymnosperms, angiosperms\n",
        "herb                 | 3.047%       |  a plant lacking a permanent woody stem; many are flowering garden plants or potherbs; some having medicinal properties; some are pests\n",
        "gramineous_plant     | 2.829%       |  cosmopolitan herbaceous or woody plants with hollow jointed stems and long narrow leaves\n",
        "grass                | 2.810%       |  narrow-leaved green herbage: grown as lawns; used as pasture for grazing animals; cut and dried as hay\n",
        "cereal               | 2.680%       |  grass whose starchy grains are used as food: wheat; rice; rye; oats; maize; buckwheat; millet\n",
        "commercial_enterprise | 2.488%       |  the activity of providing goods and services involving financial and commercial and industrial aspects\n",
        "financial_gain       | 2.477%       |  the amount of monetary gain\n",
        "income               | 2.455%       |  the financial gain (earned or unearned) accruing over a given period of time\n",
        "communicator         | 2.440%       |  a person who communicates with others\n",
        "worker               | 2.077%       |  a person who works at a specific occupation\n",
        "division             | 1.981%       |  an administrative unit in government or business\n",
        "entertainer          | 1.947%       |  a person who tries to please or amuse\n",
        "North_American_country | 1.936%       |  any country on the North American continent\n",
        "net_income           | 1.910%       |  the excess of revenues over outlays in a given period of time (including depreciation and other non-cash expenses)\n",
        "reproductive_structure | 1.896%       |  the parts of a plant involved in its reproduction\n",
        "accumulation         | 1.892%       |  (finance) profits that are not paid out as dividends but are added to the capital base of the corporation\n",
        "performer            | 1.881%       |  an entertainer who performs a dramatic or musical work for an audience\n",
        "fruit                | 1.877%       |  the ripened reproductive body of a seed plant\n",
        "motion               | 1.825%       |  the act of changing location from one place to another\n",
        "seed                 | 1.777%       |  a small hard fruit\n",
        "capitalist           | 1.736%       |  a person who invests capital in a business (especially a large business)\n",
        "industry             | 1.714%       |  the organized action of making of goods and services for sale\n",
        "federal_government   | 1.710%       |  a government with strong central powers\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Analysis\n",
      "\n",
      "I really like this one, partly because traversing concepts in wordnet just seems cool. What I like about this is how it adds some insight to the simple noun counting from my first method and allows nouns that may not be common individually but are linked in cocept to bubble up. So for the mystery text, I still see commerce as a high ranking concept but there is also a lot about plants and food stuffs. So I'm guessing it has something to do with agricultural concepts. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Finding Proper Nouns\n",
      "\n",
      "For my corpus (and many others), pulling out noun phrases could be very important. I used chunking for this. This wasn't the most exciting algorithm but it's an important one and produced more useful results than my other experiments. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_propnoun_fd(sents):\n",
      "    \"\"\"\n",
      "    Finds proper nouns from tagged sentences and returns a frequency distribution of those nouns.\n",
      "    \"\"\"\n",
      "    grammar = r\"\"\"\n",
      "        NPROP: {<N+P>+|<N+P><IN.*|DT.*><N+P>+}\n",
      "        # realized that using the pos tagger made propper nouns NNP while in brown they are NP, hence the N+P\n",
      "    \"\"\"\n",
      "\n",
      "    noun_parser = nltk.RegexpParser(grammar)\n",
      "    \n",
      "    trees = [t for s in sents for t in noun_parser.parse(s).subtrees() if t.node == \"NPROP\"]\n",
      "    fd = nltk.FreqDist([\" \".join([w[0] for w in t]) for t in trees])\n",
      "    return fd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "debate_fd_np = get_propnoun_fd(debate_sents)\n",
      "fd_view(debate_fd_np)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "America         |847             |8.844%          \n",
        "Congress        |420             |4.386%          \n",
        "Bush            |332             |3.467%          \n",
        "Iraq            |309             |3.226%          \n",
        "John            |261             |2.725%          \n",
        "Iran            |204             |2.130%          \n",
        "Kennedy         |202             |2.109%          \n",
        "Washington      |199             |2.078%          \n",
        "Republican      |190             |1.984%          \n",
        "Carter          |189             |1.973%          \n",
        "Ford            |172             |1.796%          \n",
        "Jim             |139             |1.451%          \n",
        "Clinton         |137             |1.431%          \n",
        "Israel          |129             |1.347%          \n",
        "Nixon           |126             |1.316%          \n",
        "China           |123             |1.284%          \n",
        "Bob             |121             |1.263%          \n",
        "George Bush     |111             |1.159%          \n",
        "Gore            |109             |1.138%          \n",
        "Bill Clinton    |99              |1.034%          \n",
        "October         |97              |1.013%          \n",
        "Texas           |93              |0.971%          \n",
        "U.S.            |93              |0.971%          \n",
        "Russia          |87              |0.908%          \n",
        "Al              |81              |0.846%          \n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brown_fd_np = get_propnoun_fd(brown_sents)\n",
      "fd_view(brown_fd_np)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "Mr.             |51              |1.139%          \n",
        "U.S.            |44              |0.983%          \n",
        "Kennedy         |41              |0.916%          \n",
        "Mantle          |41              |0.916%          \n",
        "Dallas          |37              |0.827%          \n",
        "Laos            |36              |0.804%          \n",
        "Palmer          |36              |0.804%          \n",
        "Maris           |33              |0.737%          \n",
        "Congo           |30              |0.670%          \n",
        "Washington      |29              |0.648%          \n",
        "Player          |28              |0.626%          \n",
        "March           |27              |0.603%          \n",
        "Texas           |25              |0.559%          \n",
        "May             |24              |0.536%          \n",
        "Portland        |21              |0.469%          \n",
        "Congress        |19              |0.424%          \n",
        "April           |18              |0.402%          \n",
        "Jr.             |17              |0.380%          \n",
        "Republican      |17              |0.380%          \n",
        "Hughes          |16              |0.357%          \n",
        "Khrushchev      |15              |0.335%          \n",
        "Moscow          |15              |0.335%          \n",
        "Houston         |14              |0.313%          \n",
        "Mr. Kennedy     |14              |0.313%          \n",
        "San Francisco   |14              |0.313%          \n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myst_fd_np = get_propnoun_fd(myst_sents)\n",
      "fd_view(myst_fd_np)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Word            |Count           |Frequency       \n",
        "=========================================================\n",
        "said.           |447             |5.761%          \n",
        "Japan           |180             |2.320%          \n",
        "U.S.            |176             |2.268%          \n",
        "month.          |109             |1.405%          \n",
        "April           |95              |1.224%          \n",
        "Fed             |75              |0.967%          \n",
        "March           |72              |0.928%          \n",
        "February        |71              |0.915%          \n",
        "January         |55              |0.709%          \n",
        "Bank            |51              |0.657%          \n",
        "year.           |47              |0.606%          \n",
        "May             |45              |0.580%          \n",
        "added.          |43              |0.554%          \n",
        "Brazil          |40              |0.516%          \n",
        "Paris           |40              |0.516%          \n",
        "Ecus            |39              |0.503%          \n",
        "New York        |39              |0.503%          \n",
        "Tokyo           |39              |0.503%          \n",
        "West Germany    |39              |0.503%          \n",
        "India           |36              |0.464%          \n",
        "dlrs.           |34              |0.438%          \n",
        "June            |33              |0.425%          \n",
        "pct.            |33              |0.425%          \n",
        "yen.            |33              |0.425%          \n",
        "September       |30              |0.387%          \n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Analysis\n",
      "\n",
      "Of these collections, this one is most useful for my corpus because pulling out the names of candidates and places they are talking about is very important. It does add some value to the other Corpora. For Brown it lists some important subjects. For the mystery text, it makes me think it is about Japan - U.S. relations over a period of time. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Generating output for mystery.txt\n",
      "\n",
      "I am using the concept traverser to generate output for this. The file will contain the first lemma for each synset along with its frequency. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out_file = open(\"mystery.txt\", \"w\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# realizing it would have been smart to make my print functions return strings instead of just printing\n",
      "\n",
      "def concept_csv(concepts, n=DISPLAY_LIM):\n",
      "    \"Creates a c separated string for n items in concept list\"\n",
      "    \n",
      "    out = []\n",
      "    out.append(\"{},{}\".format(\"Concept\", \"Concept_Freq\"))\n",
      "    for s in concepts[0:n]:\n",
      "        out.append(\"{},{:.3}\".format(s[0].lemma_names[0], s[1]))\n",
      "    \n",
      "    return \"\\n\".join(out)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out_file.write(concept_csv(myst_concepts, 100))\n",
      "out_file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    }
   ],
   "metadata": {}
  }
 ]
}